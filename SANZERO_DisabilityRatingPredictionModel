# -*- coding: utf-8 -*-
"""SANZERO_DisabilityRatingPredictionModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EyAASyvkIBPBHgP15PbcKMkJHAsQrMMm
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install kmodes

"""# **DNN 기반 예측 모델 + 번들 생성**"""

# -*- coding: utf-8 -*-
"""
2-Stage 파이프라인 (K-Prototypes 군집 기반 새중증도, 누설 방지, 소프트 소속도)
Stage-1: (나이, 부상 부위, 부상 종류, 재해유형) -> k=6 군집 → 소프트 소속확률(clu_p0..5) + 기대값(clu_exp)
Stage-2: 기존 6입력 + Stage-1 출력 → 장해등급(1~15) 회귀

설계 원칙
- Stage-1(비지도): K-Prototypes로 범주+수치 혼합 군집 → 거리행렬 직접계산(softmax)
- 일관 라벨링: 훈련 폴드의 '원 나이 평균'이 작은 순으로 군집을 0..5에 매핑
- Stage-2: 범주형(원핫), 순서/연속(표준화) — 나이, 치료기간, clu_exp, clu_p*
"""

import os, re, warnings, random
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, cohen_kappa_score
from sklearn.metrics import mean_squared_error as _mse

def safe_rmse(y_true, y_pred):
    # 신/구 버전 모두 호환
    try:
        return _mse(y_true, y_pred, squared=False)
    except TypeError:
        return np.sqrt(_mse(y_true, y_pred))

# -----------------------------
# 경로
# -----------------------------
BASE = "/content/drive/MyDrive/sanzero"
PATHS = {
    "pswci2":   os.path.join(BASE, "pswci2_2018-2022_main.csv"),
    "pswci3":   os.path.join(BASE, "pswci3_2023.csv"),
    "colmap":   os.path.join(BASE, "변수명_최종.csv"),
    "valuemap": os.path.join(BASE, "입력값의미_최종.csv"),  # (옵션)
    "bundle":   os.path.join(BASE, "sanzero_2stage_kproto.joblib"),
}
print("[PATH CHECK]", {k: os.path.exists(v) for k,v in PATHS.items()})

# 재현성
np.random.seed(42)
random.seed(42)

# -----------------------------
# 표준 변수 세트
# -----------------------------
BASE6 = ["산업 분류","나이","성별","부상 부위","부상 종류","치료 기간"]
ACCIDENT = "재해유형"
STAGE1_INPUTS = ["나이","부상 부위","부상 종류", ACCIDENT]
TARGET = "장해등급"

FORBIDDEN_OLD_SEVERITY = {"disa056", "disa016"}

# -----------------------------
# 매핑 로더
# -----------------------------
STD_ALIASES = {
    "산업 분류": ["산업 분류","산업분류"],
    "나이": ["나이","연령","연령대","age"],
    "성별": ["성별","gender","Gender","sex"],
    "부상 부위": ["부상 부위","부상부위","injurypart"],
    "부상 종류": ["부상 종류","부상종류","injurytype"],
    "치료 기간": ["치료 기간","치료기간","con6","con16"],
    "재해유형": ["재해유형","재해 유형","사고유형","accident","accident_type"],
    "장해등급": ["장해등급","장해 등급","disa0115","disa0515"],
}

def _normalize_std_colname(c: str) -> str:
    c = str(c).strip()
    for std, aliases in STD_ALIASES.items():
        if c == std or c in aliases:
            return std
    return c

def load_column_map(csv_path: str, required_cols: list) -> dict:
    df = pd.read_csv(csv_path)
    df = df.rename(columns={col: _normalize_std_colname(col) for col in df.columns})
    key_col = df.columns[0]  # 첫 컬럼: 데이터셋 식별자
    cmap = {}
    for _, r in df.iterrows():
        ds = str(r[key_col]).strip()
        if not ds or ds.lower()=="nan":
            continue
        row_map = {}
        for k in required_cols:
            if k in df.columns:
                row_map[k] = r.get(k)
            else:
                found = None
                for alias in STD_ALIASES.get(k, []):
                    if alias in df.columns:
                        found = r.get(alias); break
                row_map[k] = found
        cmap[ds] = row_map
    return cmap

REQ_COLS = BASE6 + [ACCIDENT, TARGET]
COL_MAP  = load_column_map(PATHS["colmap"], REQ_COLS)

def _resolve_ds_key(cmap: dict, hint: str) -> str:
    if hint in cmap: return hint
    for k in cmap.keys():
        if hint in k: return k
    return next(iter(cmap.keys()))

# -----------------------------
# 데이터 로드
# -----------------------------
def _to_int(x):
    if pd.isna(x): return np.nan
    m = re.search(r"(-?\d+)", str(x).strip())
    return int(m.group(1)) if m else np.nan

def load_dataset(csv_path: str, cmap_row: dict, dsname: str) -> pd.DataFrame:
    usecols = []
    for std, src in cmap_row.items():
        if std in REQ_COLS and isinstance(src, str):
            base = src.strip()
            if base and base.lower() not in FORBIDDEN_OLD_SEVERITY:
                usecols.append(base)
    usecols = list(set(usecols))

    df = pd.read_csv(csv_path, usecols=usecols, low_memory=False)
    out = pd.DataFrame()
    for std, src in cmap_row.items():
        if std not in REQ_COLS: continue
        if isinstance(src, str) and src in df.columns:
            out[std] = df[src].map(_to_int)
        else:
            out[std] = np.nan

    out[TARGET] = out[TARGET].apply(lambda v: v if (isinstance(v,(int,float)) and 1<=v<=15) else np.nan)
    out["__ds"] = dsname
    return out[REQ_COLS + ["__ds"]]

key2 = _resolve_ds_key(COL_MAP, "pswci2_2018-2022_main")
key3 = _resolve_ds_key(COL_MAP, "pswci3_2023")
print("[USING keys] ", key2, "|", key3)

d2 = load_dataset(PATHS["pswci2"], COL_MAP[key2], "pswci2")
d3 = load_dataset(PATHS["pswci3"], COL_MAP[key3], "pswci3")
raw = pd.concat([d2, d3], ignore_index=True)

# -----------------------------
# 결측 보정 & 타입
# -----------------------------
df = raw.dropna(subset=[TARGET]).copy().reset_index(drop=True)
for c in (BASE6 + [ACCIDENT]):
    mode = df[c].mode(dropna=True)
    fillv = int(mode.iloc[0]) if len(mode)>0 and not pd.isna(mode.iloc[0]) else 0
    df[c] = df[c].fillna(fillv).astype(int)

# ============================================================
# Stage-1 (비지도): K-Prototypes k=6 → 소프트 소속확률 + 기대값
# ============================================================
try:
    from kmodes.kprototypes import KPrototypes
except Exception as e:
    raise ImportError("kmodes 패키지가 필요합니다. `pip install kmodes` 로 설치하세요.")

K = 6
CAT_IDX = [1, 2, 3]  # [부상 부위, 부상 종류, 재해유형]
NUM_IDX = [0]        # [나이(스케일)]

def _prep_X1(df_sub: pd.DataFrame, age_scaler: MinMaxScaler) -> np.ndarray:
    """나이는 0~1 스케일, 범주는 int → object 배열 (n,4)"""
    a = age_scaler.transform(df_sub[["나이"]].values.astype(float))
    p = df_sub["부상 부위"].astype(int).values.reshape(-1,1)
    t = df_sub["부상 종류"].astype(int).values.reshape(-1,1)
    acc = df_sub[ACCIDENT].astype(int).values.reshape(-1,1)
    X = np.concatenate([a, p, t, acc], axis=1).astype(object)
    return X

def _mode1d_int(a: np.ndarray) -> int:
    vals, cnts = np.unique(a, return_counts=True)
    return int(vals[np.argmax(cnts)])

def _compute_prototypes(X_num, X_cat, labels, K):
    """수치: 평균, 범주: 최빈. 비어있으면 전체 평균/최빈."""
    n_num = X_num.shape[1]
    n_cat = X_cat.shape[1]
    proto_num = np.zeros((K, n_num), dtype=float)
    proto_cat = np.zeros((K, n_cat), dtype=int)
    for k in range(K):
        mask = (labels == k)
        if mask.any():
            proto_num[k, :] = X_num[mask].mean(axis=0)
            for j in range(n_cat):
                proto_cat[k, j] = _mode1d_int(X_cat[mask, j].astype(int))
        else:
            proto_num[k, :] = X_num.mean(axis=0)
            for j in range(n_cat):
                proto_cat[k, j] = _mode1d_int(X_cat[:, j].astype(int))
    return proto_num, proto_cat

def _pairwise_dissim(X_num, X_cat, proto_num, proto_cat, gamma: float):
    """D(x,c) = ||x_num - mu_c||^2 + gamma * #(x_cat != mode_c)"""
    D_num = ((X_num[:, None, :] - proto_num[None, :, :]) ** 2).sum(axis=2)
    D_cat = (X_cat[:, None, :] != proto_cat[None, :, :]).sum(axis=2)
    return D_num + float(gamma) * D_cat

def _kproto_fit_and_soft(X1_tr, X1_va, age_tr_raw, K=6, random_state=42, tau=1.0):
    """
    KPrototypes 적합 → (훈련)라벨 → 프로토타입 산출 → (검증)거리행렬 → softmax 확률.
    군집 라벨은 '원 나이 평균' 오름차순으로 0..5 고정.
    """
    kproto = KPrototypes(n_clusters=K, init='Huang', n_init=5,
                         max_iter=100, verbose=0, random_state=random_state)
    kproto.fit(X1_tr, categorical=CAT_IDX)
    labels_tr = kproto.labels_.astype(int)

    # 라벨 정렬(원 나이 평균 기준)
    age_means = np.array([
        age_tr_raw[labels_tr == j].mean() if np.any(labels_tr == j) else np.inf
        for j in range(K)
    ])
    order = np.argsort(age_means)

    # 프로토타입 직접계산
    Xnum_tr = X1_tr[:, NUM_IDX].astype(float)
    Xcat_tr = X1_tr[:, CAT_IDX].astype(int)
    proto_num, proto_cat = _compute_prototypes(Xnum_tr, Xcat_tr, labels_tr, K)

    # gamma
    gamma = getattr(kproto, 'gamma', getattr(kproto, 'gamma_', None))
    if gamma is None:
        gamma = float(np.mean(np.std(Xnum_tr, axis=0))) if Xnum_tr.size else 1.0

    # 검증 거리 → softmax(-D/tau)
    Xnum_va = X1_va[:, NUM_IDX].astype(float)
    Xcat_va = X1_va[:, CAT_IDX].astype(int)
    D = _pairwise_dissim(Xnum_va, Xcat_va, proto_num, proto_cat, gamma)
    D = D[:, order]
    logits = -(D - D.min(axis=1, keepdims=True)) / float(tau)
    q = np.exp(logits); q /= q.sum(axis=1, keepdims=True) + 1e-12
    expv = (q * np.arange(K)[None, :]).sum(axis=1)

    artifacts = {
        "kproto": kproto,          # 참고용(예측 시 직접거리 사용)
        "order": order,
        "gamma": float(gamma),
        "proto_num": proto_num,
        "proto_cat": proto_cat,
        "tau": float(tau),
    }
    return q, expv, artifacts

# OOF 생성
N_FOLDS = 5
kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

clu_p = np.full((len(df), K), np.nan, dtype=float)
clu_exp = np.full(len(df), np.nan, dtype=float)

idx_all = np.arange(len(df))
for fold, (tr_pos, va_pos) in enumerate(kf.split(idx_all), 1):
    X1_tr_df = df.iloc[tr_pos][STAGE1_INPUTS].copy()
    X1_va_df = df.iloc[va_pos][STAGE1_INPUTS].copy()

    age_scaler_f = MinMaxScaler().fit(X1_tr_df[["나이"]].values.astype(float))
    X1_tr = _prep_X1(X1_tr_df, age_scaler_f)
    X1_va = _prep_X1(X1_va_df, age_scaler_f)

    q_va, exp_va, arts = _kproto_fit_and_soft(
        X1_tr, X1_va, age_tr_raw=X1_tr_df["나이"].values.astype(float),
        K=K, random_state=42, tau=1.0
    )
    clu_p[va_pos, :] = q_va
    clu_exp[va_pos]  = exp_va

# 누락 보정
nan_mask = np.isnan(clu_exp)
if nan_mask.any():
    fill = np.zeros(K, dtype=float); fill[2] = 1.0
    clu_p[nan_mask, :] = fill
    clu_exp[nan_mask]  = 2.0

for k in range(K):
    df[f"clu_p{k}"] = clu_p[:, k]
df["clu_exp"] = clu_exp.astype(float)

# 전체 데이터로 Stage-1 “추론용” 파라미터 학습/저장
age_scaler_full = MinMaxScaler().fit(df[["나이"]].values.astype(float))
X1_all = _prep_X1(df[STAGE1_INPUTS].copy(), age_scaler_full)

kproto_full = KPrototypes(n_clusters=K, init='Huang', n_init=5,
                          max_iter=100, verbose=0, random_state=42)
kproto_full.fit(X1_all, categorical=CAT_IDX)
labels_full = kproto_full.labels_.astype(int)

age_means_full = np.array([
    df.loc[labels_full == j, "나이"].astype(float).mean() if np.any(labels_full==j) else np.inf
    for j in range(K)
])
order_full = np.argsort(age_means_full)

Xnum_full = X1_all[:, NUM_IDX].astype(float)
Xcat_full = X1_all[:, CAT_IDX].astype(int)
proto_num_full, proto_cat_full = _compute_prototypes(Xnum_full, Xcat_full, labels_full, K)
gamma_full = getattr(kproto_full, 'gamma', getattr(kproto_full, 'gamma_', None))
if gamma_full is None:
    gamma_full = float(np.mean(np.std(Xnum_full, axis=0))) if Xnum_full.size else 1.0

# ============================================================
# Stage-2: 기존 6입력 + clu_exp + clu_p* → 장해등급 회귀
# ============================================================
CAT2 = ["산업 분류","성별","부상 부위","부상 종류"]  # 원핫
ORD2 = ["나이","치료 기간"]                         # 순서형(스케일링)
CLU_FEATS = ["clu_exp"] + [f"clu_p{k}" for k in range(K)]  # 연속(스케일링)
FEAT2 = CAT2 + ORD2 + CLU_FEATS

y_all = df[TARGET].astype(float).values
y_strat = np.clip(np.rint(y_all), 1, 15).astype(int)
X2_all = df[FEAT2].copy()

try:
    ohe2 = OneHotEncoder(handle_unknown="ignore", sparse_output=False, dtype=np.float32)
except TypeError:
    ohe2 = OneHotEncoder(handle_unknown="ignore", sparse=False, dtype=np.float32)

pre2 = ColumnTransformer(
    transformers=[
        ("cat", ohe2, CAT2),
        ("num", StandardScaler(), ORD2 + CLU_FEATS),
    ],
    remainder="drop",
    verbose_feature_names_out=False
)

X2_tr_raw, X2_te_raw, y_tr, y_te = train_test_split(
    X2_all, y_all, test_size=0.30, random_state=42, stratify=y_strat
)
X2_tr = pre2.fit_transform(X2_tr_raw)
X2_te = pre2.transform(X2_te_raw)

# -----------------------------
# Keras DNN (Huber + L2), 폴백: sklearn-MLP
# -----------------------------
backend = "keras"
try:
    import tensorflow as tf
    from tensorflow.keras import Sequential, regularizers
    from tensorflow.keras.layers import Dense, Dropout, Input
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

    tf.random.set_seed(42)
    model2 = Sequential([
        Input(shape=(X2_tr.shape[1],)),
        Dense(768, activation='relu', kernel_regularizer=regularizers.l2(1e-5)),
        Dropout(0.15),
        Dense(320, activation='relu', kernel_regularizer=regularizers.l2(1e-5)),
        Dropout(0.05),
        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-5)),
        Dense(1)
    ])
    model2.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
                   loss=tf.keras.losses.Huber(delta=1.8))
    cbs = [
        EarlyStopping(patience=10, restore_best_weights=True, monitor="val_loss"),
        ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-5, monitor="val_loss"),
    ]
    model2.fit(X2_tr, y_tr, epochs=180, batch_size=512, validation_split=0.1, verbose=1, callbacks=cbs)
    y_pred = model2.predict(X2_te, verbose=0).flatten()
except Exception as e:
    from sklearn.neural_network import MLPRegressor
    backend = "sklearn-mlp"
    print("[WARN] Keras 사용 불가, MLP로 폴백:", e)
    model2 = MLPRegressor(hidden_layer_sizes=(768,320,128), activation='relu',
                          solver='adam', alpha=1e-4, batch_size=512,
                          learning_rate_init=1e-3, max_iter=220, random_state=42, verbose=False)
    model2.fit(X2_tr, y_tr)
    y_pred = model2.predict(X2_te)

# -----------------------------
# 평가
# -----------------------------
y_hat = np.clip(np.rint(y_pred), 1, 15)
mae  = mean_absolute_error(y_te, y_hat)
rmse = safe_rmse(y_te, y_hat)

y_te_i  = y_te.astype(int)
y_hat_i = y_hat.astype(int)
acc_pm1 = np.mean(np.abs(y_te_i - y_hat_i) <= 1)
acc_pm2 = np.mean(np.abs(y_te_i - y_hat_i) <= 2)
qwk     = cohen_kappa_score(y_te_i, y_hat_i, weights='quadratic')

print(f"[RESULT] backend={backend}, rows={len(df):,}, MAE={mae:.4f}, RMSE={rmse:.4f}")
print(f"[±1 정확도] {acc_pm1:.3f}")
print(f"[±2 정확도] {acc_pm2:.3f}")
print(f"[QWK(가중 카파)] {qwk:.3f}")

# -----------------------------
# 단건 예측 API (Stage-1: 저장된 프로토타입으로 직접 거리→확률)
# -----------------------------
def _stage1_soft_with_centroids(one: dict,
                                proto_num, proto_cat, gamma, order,
                                age_scaler: MinMaxScaler, tau: float = 1.0):
    x_df = pd.DataFrame([{
        "나이": int(one["나이"]),
        "부상 부위": int(one["부상 부위"]),
        "부상 종류": int(one["부상 종류"]),
        ACCIDENT: int(one[ACCIDENT]),
    }])
    X = _prep_X1(x_df, age_scaler)               # (1,4) object
    Xnum = X[:, NUM_IDX].astype(float)           # (1,1)
    Xcat = X[:, CAT_IDX].astype(int)             # (1,3)
    D = _pairwise_dissim(Xnum, Xcat, proto_num, proto_cat, gamma)  # (1,K)
    D = D[:, order]
    logits = -(D - D.min(axis=1, keepdims=True)) / float(tau)
    q = np.exp(logits); q /= q.sum(axis=1, keepdims=True) + 1e-12
    q = q[0]
    expv = float((q * np.arange(K)).sum())
    probs = {f"clu_p{k}": float(q[k]) for k in range(K)}
    return expv, probs

def predict_from_int_selection(one: dict) -> int:
    """
    one 예:
    {"산업 분류":3, "나이":4, "성별":1, "부상 부위":7, "부상 종류":2, "치료 기간":3, "재해유형":5}
    """
    expv, probs = _stage1_soft_with_centroids(
        one,
        proto_num=proto_num_full, proto_cat=proto_cat_full,
        gamma=gamma_full, order=order_full, age_scaler=age_scaler_full, tau=1.0
    )
    row = {
        "산업 분류": int(one["산업 분류"]),
        "성별": int(one["성별"]),
        "부상 부위": int(one["부상 부위"]),
        "부상 종류": int(one["부상 종류"]),
        "나이": int(one["나이"]),
        "치료 기간": int(one["치료 기간"]),
        "clu_exp": expv,
        **probs
    }
    Xrow = pre2.transform(pd.DataFrame([row], columns=FEAT2))
    if backend == "keras":
        y = float(model2.predict(Xrow, verbose=0)[0][0])
    else:
        y = float(model2.predict(Xrow)[0])
    return int(np.clip(np.rint(y), 1, 15))

# -----------------------------
# 번들 저장
# -----------------------------
try:
    import joblib
    try:
        feat_names = pre2.get_feature_names_out().tolist()
    except Exception:
        feat_names = []
    bundle = {
        "backend": backend,
        # Stage-1 (추론용 파라미터)
        "stage1_age_scaler": age_scaler_full,
        "stage1_order": order_full,
        "stage1_proto_num": proto_num_full,
        "stage1_proto_cat": proto_cat_full,
        "stage1_gamma": float(gamma_full),
        "stage1_tau": 1.0,
        "stage1_inputs": STAGE1_INPUTS,
        # Stage-2
        "stage2_preproc": pre2,
        "stage2_features": FEAT2,
        "stage2_model": model2,
        # 메타
        "metrics": {
            "mae": float(mae),
            "rmse": float(rmse),
            "acc_pm1": float(acc_pm1),
            "acc_pm2": float(acc_pm2),
            "qwk": float(qwk),
        },
        "feature_names_after_pre2": feat_names,
        "note": "비지도 Stage-1(K-Prototypes) 기반 소프트 소속도 + 기대값을 Stage-2 회귀 입력으로 사용. 구중증도(disa056/016) 미사용.",
    }
    joblib.dump(bundle, PATHS["bundle"])
    print(f"[SAVED] {PATHS['bundle']}")
except Exception as e:
    print("[WARN] 모델 번들 저장 생략:", e)

"""# **자연어 처리 기반 장해등급표 적용 + 기존 모델 번들 사용**"""

# -*- coding: utf-8 -*-
"""
통합 파이프라인 (정확문구 매칭 + BERT 유사도 + 회귀 폴백)
요청 반영:
- 규정 매칭 성공: "<매칭문구> => 장해등급은 X급 Y호입니다" (호가 없으면 "X급"만)
- 규정 매칭 실패(모델 폴백): "모델 예측: 장해등급은 X급"
- 최종 출력에 이진변수 is_rule_match 추가(규정 매칭=1, 모델 폴백=0)
"""

import os, re, json, hashlib
from dataclasses import dataclass
from typing import Optional, Tuple, Dict, Any, List
import numpy as np
import pandas as pd

# --------------------------
# 경량 정규화 + 정확매칭용 캐노니컬 변환
# --------------------------
def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        s = "" if s is None else str(s)
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

CANON_REPL = (
    ("근로자", "사람"),
    ("노동자", "사람"),
    ("노무자", "사람"),
)

def canonicalize_for_exact(s: str) -> str:
    s = normalize_text(s)
    for a,b in CANON_REPL:
        s = s.replace(a, b)
    return s

# --------------------------
# 등급표 로더 (콤마·유사 구분자 분해/확장 + 캐노니컬 맵 생성)
# --------------------------
@dataclass
class GradeTable:
    path: str
    text_col_candidates: Tuple[str, ...] = ("장해 내용", "장해내용", "내용", "설명")
    grade_col_candidates: Tuple[str, ...] = ("장해등급", "장해 등급", "등급", "grade")
    severity_col_candidates: Tuple[str, ...] = ("심각도", "중증도", "severity")

    def _find_col(self, df: pd.DataFrame, cands: Tuple[str, ...]) -> Optional[str]:
        cols_lower = {c.lower(): c for c in df.columns}
        for k in cands:
            if k in df.columns: return k
            lk = k.lower()
            for lc, orig in cols_lower.items():
                if lk == lc or lk in lc:
                    return orig
        return None

    def load(self) -> pd.DataFrame:
        df = pd.read_csv(self.path, low_memory=False)
        tx = self._find_col(df, self.text_col_candidates)
        gr = self._find_col(df, self.grade_col_candidates)
        sv = self._find_col(df, self.severity_col_candidates)
        if tx is None or gr is None:
            raise ValueError("장해등급표에서 '장해 내용' 또는 '장해등급' 컬럼을 찾지 못했습니다.")

        base = df[[tx, gr] + ([sv] if sv is not None else [])].copy()
        base.rename(columns={tx:"장해 내용", gr:"장해등급"}, inplace=True)
        if sv is not None: base.rename(columns={sv:"심각도"}, inplace=True)

        # 1) 콤마/유사 구분자 분리 → explode
        SPLIT_RE = r"\s*[,，、/;]\s*"
        base["장해 내용"] = base["장해 내용"].astype(str).str.split(SPLIT_RE)
        base = base.explode("장해 내용", ignore_index=True)

        # 2) 정규화 + 공백 제거
        base["장해 내용"] = base["장해 내용"].astype(str).map(normalize_text)
        base = base[base["장해 내용"].str.len() > 0]

        # 3) 중복 제거
        keep_cols = ["장해 내용", "장해등급"] + (["심각도"] if "심각도" in base.columns else [])
        base = base.drop_duplicates(subset=keep_cols).reset_index(drop=True)

        # 4) 정확매칭용 캐노니컬 키
        base["__canon"] = base["장해 내용"].map(canonicalize_for_exact)
        return base

# --------------------------
# (옵션) 도메인 가드 사전(기본 OFF)
# --------------------------
INJURY_LEX: Dict[int, List[str]] = {
    1: ["손", "손가락", "수지", "지골", "수부"],
    2: ["팔", "상지", "상완", "전완", "팔꿈치"],
    3: ["다리", "하지", "발", "족지", "슬관절", "무릎"],
    4: ["허리", "요추", "요부", "척추", "등"],
    5: ["눈", "시력", "안구", "시야", "각막"],
    6: ["귀", "청력", "이명", "고막"],
    7: ["입", "구강", "치아", "턱", "구순"],
    8: ["흉부", "복부", "내장", "장기"],
}
INJURYTYPE_LEX = {
    1: ["절단", "단지", "잘림", "절단상", "절단사고", "손가락이 잘렸", "끊어졌"],
    2: ["골절", "부러졌", "금이 갔", "골편", "골절상"],
    3: ["염좌", "삐끗", "좌상", "연부조직 손상", "인대 손상"],
    4: ["화상", "화끈거림", "열상", "데임", "화상상"],
    5: ["절창", "창상", "베임", "자상", "절상"],
    6: ["타박", "멍", "타박상", "부딪혀", "충격"],
}
def _domain_boost(desc: str, cand: str, ints: Dict[str,int], boost: float) -> float:
    part = ints.get("부상 부위", None)
    if part is None or part not in INJURY_LEX: return 0.0
    kws = INJURY_LEX[part]
    hit_desc = any(k in desc for k in kws)
    hit_cand = any(k in cand for k in kws)
    if hit_desc and hit_cand: return +boost
    if hit_desc and not hit_cand: return -boost/2
    return 0.0
def _type_boost(desc: str, cand: str, ints: Dict[str,int], boost: float) -> float:
    tcode = ints.get("부상 종류", None)
    if tcode is None or tcode not in INJURYTYPE_LEX: return 0.0
    kws = INJURYTYPE_LEX[tcode]
    hit_desc = any(k in desc for k in kws)
    hit_cand = any(k in cand for k in kws)
    if hit_desc and hit_cand: return +boost
    if hit_desc and not hit_cand: return -boost/2
    return 0.0

# --------------------------
# BERT 인덱서 (기본: BERT only)
# --------------------------
class BertHybridIndexer:
    def __init__(self, model_name_or_path: str,
                 cache_dir: Optional[str] = None,
                 device: Optional[str] = None,
                 batch_size: int = 256,
                 hybrid_alpha: float = 1.0,
                 allow_fallback: bool = True):
        self.model_name_or_path = model_name_or_path
        self.cache_dir = cache_dir
        self.device = device
        self.batch_size = batch_size
        self.hybrid_alpha = float(hybrid_alpha)
        self.allow_fallback = allow_fallback

        self._st_model = None
        try:
            from sentence_transformers import SentenceTransformer
            self._st_model = SentenceTransformer(model_name_or_path, device=device)
        except Exception as e:
            if not allow_fallback:
                raise RuntimeError(f"sentence-transformers 모델 로드 실패: {e}")
            self._st_model = None

        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        self._tfidf = TfidfVectorizer(analyzer="char", ngram_range=(2,5),
                                      min_df=1, max_features=30000)
        self._cosine = cosine_similarity
        self._X_tfidf = None
        self.embs = None

    def _cache_path(self, grade_table_path: str) -> str:
        p = os.path.abspath(grade_table_path)
        try:
            st = os.stat(p)
            sig = f"{p}|{st.st_mtime}|{st.st_size}|{self.model_name_or_path}"
        except Exception:
            sig = f"{p}|{self.model_name_or_path}"
        h = hashlib.md5(sig.encode("utf-8")).hexdigest()
        base = self.cache_dir if self.cache_dir else os.path.dirname(p)
        return os.path.join(base, f"emb_cache_{h}.npz")

    def fit(self, grade_df: pd.DataFrame, grade_table_path: str):
        os.makedirs(self.cache_dir or os.path.dirname(grade_table_path), exist_ok=True)
        self.db = grade_df.copy()
        self.texts = self.db["장해 내용"].astype(str).tolist()

        self._X_tfidf = self._tfidf.fit_transform(self.texts)

        if self._st_model is not None:
            self.cache_file = self._cache_path(grade_table_path)
            if os.path.exists(self.cache_file):
                try:
                    data = np.load(self.cache_file, mmap_mode="r")
                    self.embs = np.array(data["embs"])
                except Exception:
                    self.embs = self._encode(self.texts)
                    np.savez_compressed(self.cache_file, embs=self.embs)
            else:
                self.embs = self._encode(self.texts)
                np.savez_compressed(self.cache_file, embs=self.embs)

    def _encode(self, texts):
        vecs = []
        for i in range(0, len(texts), self.batch_size):
            seg = texts[i:i+self.batch_size]
            v = self._st_model.encode(seg, convert_to_numpy=True, normalize_embeddings=True)
            vecs.append(v.astype(np.float32))
        return np.vstack(vecs)

    def _bert_scores(self, q_norm: str) -> Optional[np.ndarray]:
        if self._st_model is None or self.embs is None:
            return None
        qv = self._st_model.encode([q_norm], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)[0]
        return (self.embs @ qv).astype(float)

    def _tfidf_scores(self, q_norm: str) -> np.ndarray:
        qv = self._tfidf.transform([q_norm])
        sims = self._cosine(qv, self._X_tfidf)[0]
        return sims.astype(float)

    def scores(self, text: str) -> np.ndarray:
        q = normalize_text(text)
        if len(q) == 0:
            return np.zeros(len(self.texts), dtype=float)
        s_tfidf = self._tfidf_scores(q)
        s_bert = self._bert_scores(q)
        if s_bert is None: return s_tfidf
        if self.hybrid_alpha >= 1.0: return s_bert
        if self.hybrid_alpha <= 0.0: return s_tfidf
        return self.hybrid_alpha * s_bert + (1.0 - self.hybrid_alpha) * s_tfidf

    def topk(self, text: str, k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        sims = self.scores(text)
        if sims.size == 0:
            return np.array([], dtype=int), np.array([], dtype=float)
        k = min(k, len(sims))
        idx = np.argpartition(-sims, kth=k-1)[:k]
        order = idx[np.argsort(-sims[idx])]
        return order.astype(int), sims[order].astype(float)

# --------------------------
# 번들 모델 어댑터 (Stage-1→Stage-2 추론)
# --------------------------
class Stage2ModelAdapter:
    def __init__(self, bundle_path: str):
        import joblib
        self.bundle = joblib.load(bundle_path)
        self.age_scaler = self.bundle["stage1_age_scaler"]
        self.order      = np.array(self.bundle["stage1_order"], dtype=int)
        self.proto_num  = np.array(self.bundle["stage1_proto_num"], dtype=float)
        self.proto_cat  = np.array(self.bundle["stage1_proto_cat"], dtype=int)
        self.gamma      = float(self.bundle["stage1_gamma"])
        self.tau        = float(self.bundle.get("stage1_tau", 1.0))
        self.pre2       = self.bundle["stage2_preproc"]
        self.features   = list(self.bundle["stage2_features"])
        self.model2     = self.bundle["stage2_model"]
        self.backend    = str(self.bundle.get("backend", "keras"))

    def _pairwise_dissim(self, X_num, X_cat):
        D_num = ((X_num[:, None, :] - self.proto_num[None, :, :]) ** 2).sum(axis=2)
        D_cat = (X_cat[:, None, :] != self.proto_cat[None, :, :]).sum(axis=2)
        return D_num + float(self.gamma) * D_cat

    def _prep_X1(self, one_row: Dict[str,int]) -> np.ndarray:
        a = np.array([[float(int(one_row["나이"]))]])
        a = self.age_scaler.transform(a)
        p = int(one_row["부상 부위"]); t = int(one_row["부상 종류"]); acc = int(one_row["재해유형"])
        return np.array([[a[0,0], p, t, acc]], dtype=object)

    def _stage1_soft(self, one_row: Dict[str,int]):
        X = self._prep_X1(one_row)
        Xnum = X[:, [0]].astype(float)
        Xcat = X[:, [1,2,3]].astype(int)
        D = self._pairwise_dissim(Xnum, Xcat)[:, self.order]
        logits = -(D - D.min(axis=1, keepdims=True)) / float(self.tau)
        q = np.exp(logits); q /= (q.sum(axis=1, keepdims=True) + 1e-12)
        q = q[0]
        expv = float((q * np.arange(q.shape[0])).sum())
        probs = {f"clu_p{k}": float(q[k]) for k in range(q.shape[0])}
        return expv, probs

    def predict_grade(self, one_row: Dict[str,int]) -> int:
        expv, probs = self._stage1_soft(one_row)
        row = {
            "산업 분류": int(one_row["산업 분류"]),
            "성별": int(one_row["성별"]),
            "부상 부위": int(one_row["부상 부위"]),
            "부상 종류": int(one_row["부상 종류"]),
            "나이": int(one_row["나이"]),
            "치료 기간": int(one_row["치료 기간"]),
            "clu_exp": expv,
        }
        row.update(probs)
        Xrow = self.pre2.transform(pd.DataFrame([row], columns=self.features))
        if self.backend == "keras":
            y = float(self.model2.predict(Xrow, verbose=0)[0][0])
        else:
            y = float(self.model2.predict(Xrow)[0])
        return int(np.clip(np.rint(y), 1, 15))

# --------------------------
# 통합 파이프라인
# --------------------------
@dataclass
class IntegratedPipeline:
    grade_table_path: str = "/content/drive/MyDrive/sanzero/장해등급표_업데이트.csv"
    model_bundle_path: str = "/content/drive/MyDrive/sanzero/sanzero_2stage_kproto.joblib"

    model_name_or_path: str = "jhgan/ko-sroberta-multitask"
    device: Optional[str] = None
    cache_dir: str = "/content/drive/MyDrive/sanzero"

    similarity_threshold: float = 0.72
    margin: float = 0.04
    hybrid_alpha: float = 1.0
    allow_fallback: bool = True

    use_guards: bool = False
    guard_part_weight: float = 0.03
    guard_type_weight: float = 0.02

    def __post_init__(self):
        gtab = GradeTable(self.grade_table_path)
        self.grade_df = gtab.load()
        self._db_texts = self.grade_df["장해 내용"].tolist()

        # 정확문구 캐노니컬 맵
        self._canon_map = {}
        for i, r in self.grade_df.iterrows():
            self._canon_map.setdefault(canonicalize_for_exact(r["장해 내용"]), []).append(i)

        self.indexer = BertHybridIndexer(
            model_name_or_path=self.model_name_or_path,
            cache_dir=self.cache_dir,
            device=self.device,
            hybrid_alpha=self.hybrid_alpha,
            allow_fallback=self.allow_fallback
        )
        self.indexer.fit(self.grade_df, self.grade_table_path)

        self.model = Stage2ModelAdapter(self.model_bundle_path) if os.path.exists(self.model_bundle_path) else None

    def _normalize_int_inputs(self, user_input: Dict[str, Any]) -> Dict[str,int]:
        rename = {
            "산업 분류":"산업 분류","나이":"나이","성별":"성별",
            "부상 부위":"부상 부위","부상 종류":"부상 종류","치료 기간":"치료 기간",
            "재해 유형":"재해유형","재해유형":"재해유형"
        }
        row = {}
        for k_in, k_std in rename.items():
            if k_in in user_input:
                row[k_std] = int(user_input[k_in])
        needed = ["산업 분류","나이","성별","부상 부위","부상 종류","치료 기간","재해유형"]
        miss = [k for k in needed if k not in row]
        if miss: raise ValueError(f"모델 입력 누락: {miss}")
        return row

    # A) 정확문구 매칭(캐노니컬)
    def _exact_row(self, desc: str) -> Optional[pd.Series]:
        key = canonicalize_for_exact(desc)
        idxs = self._canon_map.get(key, [])
        if not idxs: return None
        return self.grade_df.iloc[int(idxs[0])]

    # B) BERT 유사도 매칭(필요시 가드 가산)
    def _text_match(self, desc: str, ints: Dict[str,int]) -> Dict[str, Any]:
        desc_n = normalize_text(desc)
        order, sims = self.indexer.topk(desc_n, k=10)
        if sims.size == 0:
            return {"matched": False, "similarity": 0.0, "rank1_text": ""}

        re_sims = []
        for i, s in zip(order, sims):
            cand = self._db_texts[int(i)]
            s2 = float(s)
            if self.use_guards:
                s2 += _domain_boost(desc_n, cand, ints, boost=self.guard_part_weight)
                s2 += _type_boost(desc_n, cand, ints, boost=self.guard_type_weight)
            re_sims.append(s2)
        re_sims = np.clip(np.array(re_sims, dtype=float), 0.0, 1.0)

        re_order = np.argsort(-re_sims)[:5]
        sims2 = re_sims[re_order]
        order2 = order[re_order]

        best = float(sims2[0])
        second = float(sims2[1]) if len(sims2) > 1 else 0.0
        ok = (best >= self.similarity_threshold) and ((best - second) >= self.margin)
        row = self.grade_df.iloc[int(order2[0])].to_dict()
        return {
            "matched": bool(ok),
            "similarity": best,
            "top2_gap": best - second,
            "matched_row": row,
            "rank1_text": self._db_texts[int(order2[0])],
            "rank1_idx": int(order2[0]),
            "rank_k_idx": order2.astype(int).tolist(),
            "rank_k_sim": [float(x) for x in sims2.tolist()],
        }

    # 메시지 포맷: 규정 매칭 성공 시(호 있으면 X급 Y호, 없으면 X급), 모델 폴백 시 X급만
    def _format_message_rule(self, text: str, grade: int, severity: Optional[Any]) -> str:
        sev = None
        if severity is not None and not (isinstance(severity, float) and np.isnan(severity)):
            try: sev = int(severity)
            except Exception: pass
        return f"{text} => 장해등급은 {int(grade)}급{(' ' + str(sev) + '호') if sev is not None else ''}입니다"

    def _format_message_model(self, grade: int) -> str:
        return f"모델 예측: 장해등급은 {int(grade)}급입니다"

    def predict(self, user_input: Dict[str, Any]) -> Dict[str, Any]:
        desc = user_input.get("장해 내용", "")
        ints = self._normalize_int_inputs(user_input)

        # 1) 정확문구 매칭
        exact = self._exact_row(desc)
        if exact is not None:
            grade = int(exact["장해등급"])
            sev = exact.get("심각도")
            message = self._format_message_rule(normalize_text(desc), grade, sev)
            return {
                "source": "text-exact",
                "matched_text": normalize_text(desc),
                "predicted_grade": grade,
                "is_rule_match": 1,
                "message": message
            }

        # 2) BERT 유사도 매칭
        match = self._text_match(desc, ints)
        row = match.get("matched_row", {}) or {}
        if match.get("matched"):
            grade = int(row.get("장해등급"))
            message = self._format_message_rule(match.get("rank1_text",""), grade, row.get("심각도"))
            return {
                "source": "text-sim",
                "similarity": float(match["similarity"]),
                "top2_gap": float(match["top2_gap"]),
                "matched_text": match.get("rank1_text"),
                "predicted_grade": grade,
                "is_rule_match": 1,
                "message": message
            }

        # 3) 규정 매칭 실패 → 모델 폴백
        if self.model is None:
            return {
                "source":"model",
                "error":"모델 번들을 찾지 못했습니다.",
                "predicted_grade": -1,
                "is_rule_match": 0,
                "message": self._format_message_model(-1),
                "candidate_text": match.get("rank1_text"),
                "candidate_similarity": float(match.get("similarity", 0.0))
            }

        pred_grade = int(self.model.predict_grade(ints))
        return {
            "source": "model",
            "predicted_grade": pred_grade,
            "is_rule_match": 0,
            "message": self._format_message_model(pred_grade),
            "candidate_text": match.get("rank1_text"),
            "candidate_similarity": float(match.get("similarity", 0.0))
        }

# --------------------------
# 데모
# --------------------------
if __name__ == "__main__":
    GTAB   = "/content/drive/MyDrive/sanzero/장해등급표_업데이트.csv"
    BUNDLE = "/content/drive/MyDrive/sanzero/sanzero_2stage_kproto.joblib"

    pipe = IntegratedPipeline(
        grade_table_path=GTAB,
        model_bundle_path=BUNDLE,
        model_name_or_path="jhgan/ko-sroberta-multitask",
        similarity_threshold=0.72,
        margin=0.04,
        hybrid_alpha=1.0,
        allow_fallback=True,
        use_guards=False
    )

    cases = [
        ("한 손의 엄지손가락과 둘째손가락외의 손가락의 지골의 일부를 잃은 사람",
         {"부상 부위":1,"부상 종류":1,"치료 기간":3,"성별":1,"나이":3,"산업 분류":2,"재해 유형":1}),
        ("프레스 작업 중 손가락 절단",
         {"부상 부위":1,"부상 종류":1,"치료 기간":2,"성별":1,"나이":3,"산업 분류":2,"재해 유형":1}),
        ("청소 중 낙상으로 허리를 삐끗했습니다.",
         {"부상 부위":4,"부상 종류":3,"치료 기간":1,"성별":1,"나이":3,"산업 분류":2,"재해 유형":1}),
    ]

    for desc, ints in cases:
        payload = {**ints, "장해 내용": desc}
        res = pipe.predict(payload)
        print("----")
        print("입력 설명:", desc)
        print(res["message"])
        print({"predicted_grade": res["predicted_grade"], "is_rule_match": res["is_rule_match"]})

"""# **배포용 통합 번들 생성**"""

# -*- coding: utf-8 -*-
"""
sanzero_integrated_bundle.py  (올인원 번들: 저장/로드/예측)

- 단계1: '장해 내용' 정확문구 매칭(canonical=간단 정규화 기반)
- 단계2: BERT 유사도 매칭 (기본 BERT only; sentence-transformers 미설치/네트워크 불가 시 TF-IDF 폴백)
- 단계3: 둘 다 불충분 시 Stage-1/2 번들 회귀 예측(기존 파이프라인 내장)

출력 규칙(요청 반영):
- 규정 매칭(정확/유사) 성공: "<매칭문구> => 장해등급은 X급 Y호입니다"
  * 호 정보가 없으면 "<매칭문구> => 장해등급은 X급입니다" (미상 문구 표시 없음)
  * rule_match = 1
- 규정 매칭 실패 → 모델 폴백: "모델 예측: 장해등급은 X급입니다"
  * rule_match = 0
- 최종적으로 결과 딕트에 최소 필수 키: {"predicted_grade": int, "rule_match": 0/1}

번들(joblib) 하나로 배포 가능:
- 정규화/확장된 장해등급표, SBERT 임베딩(사전계산), TF-IDF, Stage-2(전처리/모델/프로토타입) 포함
"""

import os, re, json, hashlib
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple
import numpy as np
import pandas as pd

# --------------------------
# 공통 유틸
# --------------------------
def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        s = "" if s is None else str(s)
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

def _split_variants(s: str) -> List[str]:
    # 콤마 위주로 변형 표현을 분해 (필요시 구분자 추가 가능: , ， 、 / ;)
    parts = re.split(r"\s*[,，、/;]\s*", str(s))
    parts = [normalize_text(p) for p in parts]
    return [p for p in parts if p]

def _to_int_or_none(x) -> Optional[int]:
    if x is None: return None
    if isinstance(x, (int, np.integer)): return int(x)
    s = str(x).strip()
    if s == "" or s.lower() == "nan": return None
    m = re.search(r"\d+", s)
    return int(m.group()) if m else None

# --------------------------
# 장해등급표 로더
# --------------------------
def load_grade_table(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path, low_memory=False)

    def _find(df, cands: Tuple[str, ...]):
        low = {c.lower(): c for c in df.columns}
        for k in cands:
            if k in df.columns: return k
            lk = k.lower()
            for lc, orig in low.items():
                if lk == lc or lk in lc:
                    return orig
        return None

    tx = _find(df, ("장해 내용","장해내용","내용","설명","description"))
    gr = _find(df, ("장해등급","장해 등급","등급","grade"))
    sv = _find(df, ("심각도","중증도","severity","호"))

    if tx is None or gr is None:
        raise ValueError("CSV에서 '장해 내용' 또는 '장해등급' 컬럼을 찾지 못했습니다.")

    base = df[[tx, gr] + ([sv] if sv is not None else [])].copy()
    base = base.rename(columns={tx:"장해 내용", gr:"장해등급", (sv or "심각도"):"심각도"})
    base["장해 내용"] = base["장해 내용"].astype(str).map(normalize_text)
    base = base.dropna(subset=["장해 내용","장해등급"]).drop_duplicates().reset_index(drop=True)
    try:
        base["장해등급"] = base["장해등급"].astype(int)
    except Exception:
        pass
    # 심각도 정수화 시도(없으면 None 유지)
    base["심각도"] = base["심각도"].map(_to_int_or_none)
    return base

# --------------------------
# 인덱서(임베딩/TF-IDF) - 번들 내 저장 대상
# --------------------------
def _encode_sbert(model_name: str, texts: List[str], device: Optional[str] = None, batch_size: int = 256) -> np.ndarray:
    from sentence_transformers import SentenceTransformer
    m = SentenceTransformer(model_name, device=device)
    vecs = []
    for i in range(0, len(texts), batch_size):
        seg = texts[i:i+batch_size]
        v = m.encode(seg, convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)
        vecs.append(v)
    return np.vstack(vecs) if vecs else np.zeros((0, 384), dtype=np.float32)

def _build_tfidf(texts: List[str]):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    vect = TfidfVectorizer(analyzer="char", ngram_range=(2,5), min_df=1, max_features=60000)
    X = vect.fit_transform(texts)
    return vect, X, cosine_similarity

# --------------------------
# Stage-2 어댑터(번들 내부에 dict로 보관)
# --------------------------
def _stage1_soft_from_bundle(one_row: Dict[str,int], b: Dict[str,Any]) -> Tuple[float, Dict[str,float]]:
    a = np.array([[float(int(one_row["나이"]))]])
    a = b["age_scaler"].transform(a)
    p = int(one_row["부상 부위"]); t = int(one_row["부상 종류"]); acc = int(one_row["재해유형"])
    Xnum = np.array([[a[0,0]]], dtype=float)
    Xcat = np.array([[p,t,acc]], dtype=int)
    D_num = ((Xnum[:, None, :] - b["proto_num"][None, :, :]) ** 2).sum(axis=2)
    D_cat = (Xcat[:, None, :] != b["proto_cat"][None, :, :]).sum(axis=2)
    D = D_num + float(b["gamma"]) * D_cat
    D = D[:, np.array(b["order"], dtype=int)]
    logits = -(D - D.min(axis=1, keepdims=True)) / float(b.get("tau", 1.0))
    q = np.exp(logits); q /= (q.sum(axis=1, keepdims=True) + 1e-12)
    q = q[0]
    expv = float((q * np.arange(q.shape[0])).sum())
    probs = {f"clu_p{k}": float(q[k]) for k in range(q.shape[0])}
    return expv, probs

def _stage2_predict_grade(one_row: Dict[str,int], b: Dict[str,Any]) -> int:
    expv, probs = _stage1_soft_from_bundle(one_row, b)
    row = {
        "산업 분류": int(one_row["산업 분류"]),
        "성별": int(one_row["성별"]),
        "부상 부위": int(one_row["부상 부위"]),
        "부상 종류": int(one_row["부상 종류"]),
        "나이": int(one_row["나이"]),
        "치료 기간": int(one_row["치료 기간"]),
        "clu_exp": expv,
    }
    row.update(probs)
    Xrow = b["pre2"].transform(pd.DataFrame([row], columns=b["features"]))
    backend = str(b.get("backend", "keras"))
    if backend == "keras":
        y = float(b["model2"].predict(Xrow, verbose=0)[0][0])
    else:
        y = float(b["model2"].predict(Xrow)[0])
    return int(np.clip(np.rint(y), 1, 15))

# --------------------------
# 통합 번들 객체
# --------------------------
@dataclass
class IntegratedBundle:
    version: str
    config: Dict[str, Any]
    # canonical 사전
    canon_texts: List[str]
    canon_grade: List[Any]
    canon_severity: List[Any]
    canon_index_of_text: Dict[str, int]  # normalized text -> index
    # 검색 인덱스
    model_name_or_path: str
    embeddings: np.ndarray               # (N, d)
    tfidf_vect: Any                      # TfidfVectorizer
    tfidf_matrix: Any                    # sparse
    tfidf_cosine: Any                    # function
    # Stage-2 파트(기존 번들 그대로 내장)
    stage2: Dict[str, Any]               # {'age_scaler','order','proto_*','gamma','tau','pre2','features','model2','backend',...}

    # -------- 저장/로드 --------
    def save(self, path: str):
        import joblib
        joblib.dump(self, path)
        return path

    @staticmethod
    def load(path: str) -> "IntegratedBundle":
        import joblib
        obj = joblib.load(path)
        return obj

    # -------- 내부 유틸 --------
    def _exact_match(self, desc: str) -> Optional[int]:
        key = normalize_text(desc)
        return self.canon_index_of_text.get(key, None)

    def _bert_topk(self, desc: str, k: int = 5) -> Tuple[np.ndarray, np.ndarray, bool]:
        # True/False: 실제 BERT 사용여부(미설치시 False)
        try:
            from sentence_transformers import SentenceTransformer
        except Exception:
            sims = self._tfidf_scores(desc)
            order = np.argsort(-sims)[:k]
            return order.astype(int), sims[order].astype(float), False

        m = SentenceTransformer(self.model_name_or_path, device=self.config.get("device"))
        qv = m.encode([normalize_text(desc)], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)[0]
        sims = (self.embeddings @ qv).astype(float)
        order = np.argsort(-sims)[:k]
        return order.astype(int), sims[order].astype(float), True

    def _tfidf_scores(self, desc: str) -> np.ndarray:
        qv = self.tfidf_vect.transform([normalize_text(desc)])
        sims = self.tfidf_cosine(qv, self.tfidf_matrix)[0]
        return sims.astype(float)

    # -------- 예측 (출력 규칙 반영) --------
    def predict(self, user_input: Dict[str, Any]) -> Dict[str, Any]:
        # 0) 입력 정규화
        rename = {
            "산업 분류":"산업 분류","나이":"나이","성별":"성별",
            "부상 부위":"부상 부위","부상 종류":"부상 종류","치료 기간":"치료 기간",
            "재해 유형":"재해유형","재해유형":"재해유형"
        }
        ints = {}
        for k_in, k_std in rename.items():
            if k_in in user_input:
                ints[k_std] = int(user_input[k_in])
        need = ["산업 분류","나이","성별","부상 부위","부상 종류","치료 기간","재해유형"]
        miss = [k for k in need if k not in ints]
        if miss:
            raise ValueError(f"모델 입력 누락: {miss}")

        desc = user_input.get("장해 내용", "") or ""
        desc_n = normalize_text(desc)

        # 1) 정확문구 매칭
        idx = self._exact_match(desc_n)
        if idx is not None:
            g = int(self.canon_grade[idx])
            s = _to_int_or_none(self.canon_severity[idx])
            if s is not None:
                msg = f"{self.canon_texts[idx]} => 장해등급은 {g}급 {s}호입니다"
            else:
                msg = f"{self.canon_texts[idx]} => 장해등급은 {g}급입니다"
            return {
                "source":"canonical",
                "predicted_grade": g,
                "rule_match": 1,            # ✅ 이진변수(규정 매칭 여부)
                "severity": s,
                "message": msg
            }

        # 2) BERT 유사도 매칭
        order, sims, used_bert = self._bert_topk(desc_n, k=5)
        best = float(sims[0]) if len(sims) else 0.0
        second = float(sims[1]) if len(sims)>1 else 0.0
        ok = (best >= float(self.config["similarity_threshold"])) and ((best - second) >= float(self.config["margin"]))
        if ok and len(order)>0:
            i = int(order[0])
            g = int(self.canon_grade[i])
            s = _to_int_or_none(self.canon_severity[i])
            if s is not None:
                msg = f"{self.canon_texts[i]} => 장해등급은 {g}급 {s}호입니다"
            else:
                msg = f"{self.canon_texts[i]} => 장해등급은 {g}급입니다"
            return {
                "source":"text-match-bert" if used_bert else "text-match-tfidf",
                "similarity": best, "top2_gap": best-second,
                "matched_text": self.canon_texts[i],
                "predicted_grade": g,
                "rule_match": 1,            # ✅ 규정 매칭으로 판단
                "severity": s,
                "message": msg
            }

        # 3) 모델 폴백(Stage-1/2)
        if self.stage2 is None:
            return {
                "source":"model",
                "error":"내장 Stage-2 번들이 없습니다.",
                "predicted_grade": -1,
                "rule_match": 0,            # ✅ 폴백이므로 0
                "message": "모델 예측: 장해등급은 -1급입니다"
            }
        pred = int(_stage2_predict_grade(ints, self.stage2))
        hint_text = self.canon_texts[int(order[0])] if len(order)>0 else "(없음)"
        hint_sim  = float(sims[0]) if len(sims)>0 else 0.0
        return {
            "source":"model",
            "predicted_grade": pred,
            "rule_match": 0,                # ✅ 폴백이므로 0
            "candidate_text": hint_text,
            "candidate_similarity": hint_sim,
            "message": f"모델 예측: 장해등급은 {pred}급입니다"
        }

# --------------------------
# 번들 빌더: CSV + 기존 Stage-2(joblib) → 통합 번들 저장
# --------------------------
def build_and_save_bundle(
    grade_table_csv: str,
    stage2_bundle_joblib: Optional[str],
    out_path: str,
    model_name_or_path: str = "jhgan/ko-sroberta-multitask",
    device: Optional[str] = None,
    similarity_threshold: float = 0.72,
    margin: float = 0.04,
    precomputed_embeddings: Optional[np.ndarray] = None,
) -> str:
    import joblib
    # 1) 장해등급표 로드 & 변형표현 확장
    df = load_grade_table(grade_table_csv)
    texts: List[str] = []
    grades: List[Any] = []
    sever: List[Any] = []
    for _, r in df.iterrows():
        for v in _split_variants(r["장해 내용"]):
            texts.append(v)
            grades.append(r["장해등급"])
            sever.append(r.get("심각도", None))

    # 정규화/중복 제거
    norm = [normalize_text(t) for t in texts]
    idx_keep, seen = [], set()
    for i, t in enumerate(norm):
        if t not in seen:
            seen.add(t); idx_keep.append(i)
    texts = [texts[i] for i in idx_keep]
    grades = [grades[i] for i in idx_keep]
    sever  = [sever[i]  for i in idx_keep]
    # 심각도 정수화
    sever = [_to_int_or_none(s) for s in sever]
    canon_index = {normalize_text(t): idx for idx, t in enumerate(texts)}

    # 2) 임베딩/TF-IDF 생성
    embs = precomputed_embeddings if precomputed_embeddings is not None else _encode_sbert(model_name_or_path, texts, device=device)
    tfidf_vect, tfidf_matrix, tfidf_cos = _build_tfidf(texts)

    # 3) Stage-2 번들 주입(선택)
    stage2 = None
    if stage2_bundle_joblib and os.path.exists(stage2_bundle_joblib):
        b = joblib.load(stage2_bundle_joblib)
        stage2 = {
            "age_scaler": b["stage1_age_scaler"],
            "order":      b["stage1_order"],
            "proto_num":  b["stage1_proto_num"],
            "proto_cat":  b["stage1_proto_cat"],
            "gamma":      b["stage1_gamma"],
            "tau":        b.get("stage1_tau", 1.0),
            "pre2":       b["stage2_preproc"],
            "features":   b["stage2_features"],
            "model2":     b["stage2_model"],
            "backend":    b.get("backend","keras"),
        }

    # 4) 번들 구성/저장
    bundle = IntegratedBundle(
        version="1.0.1",
        config={"similarity_threshold": float(similarity_threshold), "margin": float(margin), "device": device},
        canon_texts=texts,
        canon_grade=grades,
        canon_severity=sever,
        canon_index_of_text=canon_index,
        model_name_or_path=model_name_or_path,
        embeddings=embs.astype(np.float32),
        tfidf_vect=tfidf_vect,
        tfidf_matrix=tfidf_matrix,
        tfidf_cosine=tfidf_cos,
        stage2=stage2
    )
    return bundle.save(out_path)


if __name__ == "__main__":
    GTAB   = "/content/drive/MyDrive/sanzero/장해등급표_업데이트.csv"
    STG2   = "/content/drive/MyDrive/sanzero/sanzero_2stage_kproto.joblib"  # 없으면 None
    OUT    = "/content/drive/MyDrive/sanzero/sanzero_integrated_bundle.joblib"
    MODEL  = "jhgan/ko-sroberta-multitask"

    # 1) 번들 생성/저장
    path = build_and_save_bundle(
        grade_table_csv=GTAB,
        stage2_bundle_joblib=STG2,
        out_path=OUT,
        model_name_or_path=MODEL,
        device=None,
        similarity_threshold=0.72,
        margin=0.04,
    )
    print("[SAVED]", path)

    # 2) 번들 로드 & 데모 예측
    B = IntegratedBundle.load(OUT)
    demo = {
        "부상 부위":1, "부상 종류":1, "치료 기간":2,
        "성별":1, "나이":3, "산업 분류":2, "재해 유형":1,
        "장해 내용":"프레스 작업 중 손가락 절단"
    }
    res = B.predict(demo)
    # 최소 보장 키: predicted_grade, rule_match(1=규정매칭, 0=모델)
    print(json.dumps({k:res[k] for k in ["predicted_grade","rule_match"]}, ensure_ascii=False))
    # 사람이 보기 좋게:
    print(res["message"])
